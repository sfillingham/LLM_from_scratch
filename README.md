# LLM_from_scratch
My development of an LLM following Sebastian Raschka's book, "Build a Large Language Model (from scratch)"

This will involve training a simple GPT-2 model on a small, open source dataset

## Repo File Structure

- tokenizers: Simple Tokenizers used to build intuition
- data_loaders: Pytorch based loaders for GPT-2 models
- attention_heads: Various implementations of attention heads
- model_architecture: Model implementations including transformer blocks, layer norm, and GELU activation