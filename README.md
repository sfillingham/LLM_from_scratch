# LLM_from_scratch
My development of an LLM following Sebastian Raschka's book, "Build a Large Language Model (from scratch)"

This will involve training a simple GPT-2 model on a small, open source dataset.

## Repo File Structure

- tokenizers: Simple Tokenizers used to build intuition
- data_loaders: Pytorch based data loaders for GPT-2 models
- attention_heads: Various implementations of attention heads
- model_architecture: Model implementations including transformer blocks, layer norm, dynamic Tanh, and GELU activation